{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4c456c9f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c456c9f",
        "outputId": "4f91cb7c-e046-44e7-9401-54a5f50ef62d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fake_useragent\n",
            "  Downloading fake-useragent-0.1.11.tar.gz (13 kB)\n",
            "Building wheels for collected packages: fake-useragent\n",
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-py3-none-any.whl size=13502 sha256=b7f01c7e26b995df0098296756b523e80ecd8e2e771f6290eb9d5e1454a77353\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/f7/62/50ab6c9a0b5567267ab76a9daa9d06315704209b2c5d032031\n",
            "Successfully built fake-useragent\n",
            "Installing collected packages: fake-useragent\n",
            "Successfully installed fake-useragent-0.1.11\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade fake_useragent"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2"
      ],
      "metadata": {
        "id": "DfmcHMHXFHuZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50819330-72b8-4d0f-8162-94612148bdee"
      },
      "id": "DfmcHMHXFHuZ",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting docopt>=0.6\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 3.6 MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=f9fc02c9b921a7841b8e4c1fc91d30a8a191db96bb5d392aa14f2c86e2015b6c\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n",
            "Successfully built docopt\n",
            "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f073b7ad",
      "metadata": {
        "id": "f073b7ad"
      },
      "source": [
        "# 1. Parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "864981fe",
      "metadata": {
        "id": "864981fe"
      },
      "outputs": [],
      "source": [
        "from fake_useragent import UserAgent\n",
        "import requests\n",
        "from pprint import pprint\n",
        "from bs4 import BeautifulSoup\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8267ea6e",
      "metadata": {
        "id": "8267ea6e"
      },
      "outputs": [],
      "source": [
        "session = requests.session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a315e652",
      "metadata": {
        "id": "a315e652",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d72429a1-21a5-43ef-8c9b-e9ec8aecdfed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fake_useragent:Error occurred during loading data. Trying to use cache server https://fake-useragent.herokuapp.com/browsers/0.1.11\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fake_useragent/utils.py\", line 154, in load\n",
            "    for item in get_browsers(verify_ssl=verify_ssl):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fake_useragent/utils.py\", line 99, in get_browsers\n",
            "    html = html.split('<table class=\"w3-table-all notranslate\">')[1]\n",
            "IndexError: list index out of range\n"
          ]
        }
      ],
      "source": [
        "ua = UserAgent(verify_ssl=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Собираю названия популярных в последнеее время фильмов."
      ],
      "metadata": {
        "id": "ga_z_hfeJVtm"
      },
      "id": "ga_z_hfeJVtm"
    },
    {
      "cell_type": "code",
      "source": [
        "req1 = session.get('https://www.metacritic.com/browse/movies/release-date/theaters/userscore', headers={'User-Agent': ua.random})\n",
        "page_all = req1.text\n",
        "soup_all = BeautifulSoup(page_all, 'html.parser')"
      ],
      "metadata": {
        "id": "dSMhW5Gvoix1"
      },
      "id": "dSMhW5Gvoix1",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titles = []\n",
        "for link in soup_all.find_all('a', {'class':'title'}):\n",
        "    titles.append(link.get('href'))"
      ],
      "metadata": {
        "id": "cf53wwPPo1ro"
      },
      "id": "cf53wwPPo1ro",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titles = titles[:20]\n",
        "print(titles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0arT8gUxpsU8",
        "outputId": "e41f6ac0-2c31-4c19-80d8-37d2affdf916"
      },
      "id": "0arT8gUxpsU8",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/movie/dragon-ball-super-super-hero', '/movie/take-the-night', '/movie/petrovs-flu', '/movie/block-party-2022', '/movie/both-sides-of-the-blade', '/movie/marcel-the-shell-with-shoes-on', '/movie/moonage-daydream', '/movie/official-competition', '/movie/thirteen-lives', '/movie/top-gun-maverick', '/movie/the-sea-beast', '/movie/bullet-train', '/movie/emily-the-criminal', '/movie/dinner-in-america', '/movie/a-jazzmans-blues', '/movie/barbarian', '/movie/benediction', '/movie/hustle-2022', '/movie/speak-no-evil', '/movie/fire-of-love']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для каждого фильма собираю положительные и отрицательные отзывы."
      ],
      "metadata": {
        "id": "j7ZEZ-9_Jh0W"
      },
      "id": "j7ZEZ-9_Jh0W"
    },
    {
      "cell_type": "code",
      "source": [
        "def at_all(film_name, fin_pos, fin_neg):\n",
        "  req = session.get(f'https://www.metacritic.com{film_name}/user-reviews', headers={'User-Agent': ua.random})\n",
        "  page = req.text\n",
        "  soup = BeautifulSoup(page, 'html.parser')\n",
        "  reviews_body = soup.find_all('div', {'class':'review pad_top1'})\n",
        "\n",
        "  def parser(review_piece, list1):\n",
        "    if '\"blurb blurb_expanded\"' in str(review_piece):\n",
        "      review = i.find('span', {'class':'blurb blurb_expanded'}).text\n",
        "    else:\n",
        "      review = i.find('div', {'class':'review_body'}).text\n",
        "    list1.append(review)\n",
        "    return (list1)\n",
        "\n",
        "  positives = []\n",
        "  negatives = []\n",
        "  for i in reviews_body:\n",
        "    if '\"metascore_w user large movie negative indiv\"' in str(i):\n",
        "      negatives = parser(i, negatives)\n",
        "    if '\"metascore_w user large movie positive indiv\"' in str(i):\n",
        "      positives = parser(i, positives)\n",
        "\n",
        "  fin_pos.append(positives)\n",
        "  fin_neg.append(negatives)\n",
        "\n",
        "  return fin_pos, fin_neg"
      ],
      "metadata": {
        "id": "SjhSgyWuq1l7"
      },
      "id": "SjhSgyWuq1l7",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_pos = []\n",
        "final_neg = []\n",
        "for i in titles:\n",
        "  final_pos = at_all(i, final_pos, final_neg)[0]\n",
        "  final_neg = at_all(i, final_pos, final_neg)[1]"
      ],
      "metadata": {
        "id": "RLe1F3QOsZIY"
      },
      "id": "RLe1F3QOsZIY",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_neg = final_neg[:-1:2]\n",
        "final_pos = final_pos[:-1:2]"
      ],
      "metadata": {
        "id": "OqOU9qq0s-bw"
      },
      "id": "OqOU9qq0s-bw",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def listmerge(lstlst):\n",
        "  all=[]\n",
        "  for lst in lstlst:\n",
        "    for el in lst:\n",
        "      all.append(el.strip('\\n').strip('\\xa0'))\n",
        "  return all"
      ],
      "metadata": {
        "id": "k7Jo8Av7yUWE"
      },
      "id": "k7Jo8Av7yUWE",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overal_neg = listmerge(final_neg)[:45]\n",
        "overal_pos = listmerge(final_pos)[:45]"
      ],
      "metadata": {
        "id": "kRw0t04v2FDn"
      },
      "id": "kRw0t04v2FDn",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Делю на тренировочную и тестовую выборки."
      ],
      "metadata": {
        "id": "GnD6mOyFJo8D"
      },
      "id": "GnD6mOyFJo8D"
    },
    {
      "cell_type": "code",
      "source": [
        "test_neg = overal_neg[0:5]\n",
        "test_pos = overal_pos[0:5]\n",
        "train_neg = overal_neg[5:-1]\n",
        "train_pos = overal_pos[5:-1]"
      ],
      "metadata": {
        "id": "08QumsMg6-Wn"
      },
      "id": "08QumsMg6-Wn",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Tokens and lemmas\n"
      ],
      "metadata": {
        "id": "l1dvd9D73w0t"
      },
      "id": "l1dvd9D73w0t"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from collections import Counter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_dEScmO30nu",
        "outputId": "bb54ec77-b077-405a-8a09-1a64356f8497"
      },
      "id": "0_dEScmO30nu",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "_zMvrcWb-M28"
      },
      "id": "_zMvrcWb-M28",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Токенизирую и попутно чищу от стоп-слов. Так как встречались отзывы не только на английском языке, очистила и от французских стоп-слов."
      ],
      "metadata": {
        "id": "DllFVfYwKJU9"
      },
      "id": "DllFVfYwKJU9"
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer(reviews):\n",
        "  sw = stopwords.words(['english', 'french'])\n",
        "  tokenized = []\n",
        "  for i in reviews:\n",
        "    words = [w.lower() for w in word_tokenize(i) if w.isalpha()]\n",
        "    filtered = [w for w in words if w not in sw]\n",
        "    for j in filtered:\n",
        "      tokenized.append(j)\n",
        "  return tokenized"
      ],
      "metadata": {
        "id": "BPxmHVNw-S7E"
      },
      "id": "BPxmHVNw-S7E",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neg_tok = tokenizer(train_neg)\n",
        "pos_tok = tokenizer(train_pos)"
      ],
      "metadata": {
        "id": "AQtgFpXPBW8t"
      },
      "id": "AQtgFpXPBW8t",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pymorphy2 import MorphAnalyzer\n",
        "morph = MorphAnalyzer()"
      ],
      "metadata": {
        "id": "Ke5ozyEcFCBp"
      },
      "id": "Ke5ozyEcFCBp",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Леммматизирую."
      ],
      "metadata": {
        "id": "Eh3BSR0zKX4U"
      },
      "id": "Eh3BSR0zKX4U"
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatizer(words):\n",
        "  lemmas = []\n",
        "  for word in words:\n",
        "      lemmas.append(morph.parse(word)[0].normal_form)\n",
        "  return lemmas"
      ],
      "metadata": {
        "id": "wwpRFnQZBeeN"
      },
      "id": "wwpRFnQZBeeN",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neg_lem = lemmatizer(neg_tok)\n",
        "pos_lem = lemmatizer(pos_tok)"
      ],
      "metadata": {
        "id": "cLVW7AGOBlZE"
      },
      "id": "cLVW7AGOBlZE",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Составляю частотный словарь, чтобы отфильтровать низкочастотные слова."
      ],
      "metadata": {
        "id": "ZBj-AJEmKcU2"
      },
      "id": "ZBj-AJEmKcU2"
    },
    {
      "cell_type": "code",
      "source": [
        "def freq_dict(list_of_words):\n",
        "  cnt = Counter()\n",
        "  for word in list_of_words:\n",
        "    if word.isalpha():\n",
        "      cnt[word] += 1\n",
        "  return dict(cnt)"
      ],
      "metadata": {
        "id": "ZuG_Gg7PIEl1"
      },
      "id": "ZuG_Gg7PIEl1",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neg_freqs, pos_freqs = freq_dict(neg_lem), freq_dict(pos_lem)"
      ],
      "metadata": {
        "id": "5BZcyMuGOcDK"
      },
      "id": "5BZcyMuGOcDK",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neg_freqs = {key:val for key, val in neg_freqs.items() if val >= 3}\n",
        "pos_freqs = {key:val for key, val in pos_freqs.items() if val >= 3}"
      ],
      "metadata": {
        "id": "Qi2uj__mO2NC"
      },
      "id": "Qi2uj__mO2NC",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Unique words\n"
      ],
      "metadata": {
        "id": "0JAHL6JzGJDA"
      },
      "id": "0JAHL6JzGJDA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "При помощи операции с множествами составляю два множества уникальных слов для каждого типа отзывов."
      ],
      "metadata": {
        "id": "fi2800ziKiz2"
      },
      "id": "fi2800ziKiz2"
    },
    {
      "cell_type": "code",
      "source": [
        "neg_words, pos_words = set(neg_freqs.keys()), set(pos_freqs.keys())"
      ],
      "metadata": {
        "id": "g-dhvnHvGNak"
      },
      "id": "g-dhvnHvGNak",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_neg = neg_words.difference(pos_words)\n",
        "unique_pos = pos_words.difference(neg_words)"
      ],
      "metadata": {
        "id": "gif9n_MmRZsM"
      },
      "id": "gif9n_MmRZsM",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Sentiment analysis"
      ],
      "metadata": {
        "id": "CNKQPf6AUi80"
      },
      "id": "CNKQPf6AUi80"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Составляю словарь для сравнения с результатом работы функции."
      ],
      "metadata": {
        "id": "rVaYp3-7K821"
      },
      "id": "rVaYp3-7K821"
    },
    {
      "cell_type": "code",
      "source": [
        "checklist = []\n",
        "for i in test_neg:\n",
        "  to_check = {}\n",
        "  to_check['review'] = i\n",
        "  to_check['sense'] = 0\n",
        "  checklist.append(to_check)\n",
        "for i in test_pos:\n",
        "  to_check ={}\n",
        "  to_check['review'] = i\n",
        "  to_check['sense'] = 1\n",
        "  checklist.append(to_check)"
      ],
      "metadata": {
        "id": "FApuqEB0dK4j"
      },
      "id": "FApuqEB0dK4j",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция подсчитывает, каких слов в тексте отзыва больше и в соответствии с этим определяет тип отзыва. Если \"отрицательных\" и \"положительных\" слов одинаковое количество, тексту присвается нейтральный статус (2)."
      ],
      "metadata": {
        "id": "3VRd_dG9LA6W"
      },
      "id": "3VRd_dG9LA6W"
    },
    {
      "cell_type": "code",
      "source": [
        "def sentimentor(review):\n",
        "  negs = 0\n",
        "  poss = 0\n",
        "  so = {}\n",
        "  for word in nltk.word_tokenize(review.lower()):\n",
        "    if morph.parse(word)[0].normal_form in unique_neg:\n",
        "      negs +=1\n",
        "    elif morph.parse(word)[0].normal_form in unique_pos:\n",
        "      poss +=1\n",
        "  so['review'] = review\n",
        "  so['inneg'] = negs\n",
        "  so['inpos'] = poss\n",
        "  if negs > poss:\n",
        "    so['sense'] = 0\n",
        "  elif negs < poss:\n",
        "    so['sense'] = 1\n",
        "  else:\n",
        "    so['sense'] = 2\n",
        "  return so"
      ],
      "metadata": {
        "id": "_1Gs5fXcTr5i"
      },
      "id": "_1Gs5fXcTr5i",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "whole_test = test_neg + test_pos"
      ],
      "metadata": {
        "id": "AdElcemNXuNJ"
      },
      "id": "AdElcemNXuNJ",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all = []\n",
        "for i in whole_test:\n",
        "  all.append(sentimentor(i))"
      ],
      "metadata": {
        "id": "6i9-jtIqavU1"
      },
      "id": "6i9-jtIqavU1",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_values(listik):\n",
        "  items = []\n",
        "  for i in listik:\n",
        "    items.append(i['sense'])\n",
        "  return items"
      ],
      "metadata": {
        "id": "waEI_Re8h1iR"
      },
      "id": "waEI_Re8h1iR",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подсчитываю accuracy с помощью sklearn. Передаю туда рещультаты работы функции и \"парвильные\" ответы."
      ],
      "metadata": {
        "id": "A1TiMi9NLyaD"
      },
      "id": "A1TiMi9NLyaD"
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn"
      ],
      "metadata": {
        "id": "zQTTtv20i4Rf"
      },
      "id": "zQTTtv20i4Rf",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = sklearn.metrics.accuracy_score(get_values(checklist), get_values(all))\n",
        "accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-BOHJ2GcVK-",
        "outputId": "5e39317d-64f5-4ef2-d343-24f48352db08"
      },
      "id": "b-BOHJ2GcVK-",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Another solution\n"
      ],
      "metadata": {
        "id": "ZvPLqFwllgu-"
      },
      "id": "ZvPLqFwllgu-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Один из способов - считать эмбеддинги для каждого слова. Эмбеддинги \"отрицательных\" слов скорее всего будут похожи между собой и сильно отличаться от \"положительных\". Так можно посчитать значение (что-то типа среднего) для каждого текста --> получить эмбеддинг для текста отзыва целиком. Можно посмотреть несколько таких эмбеддингов и сделать предположение вроде \"у отрицательных отзывов эмбеддинги отрицательные, а у положительных - положительные\".  \n",
        "2. Можно улучшить тщательнее подсчитывая слова. Например, можно выделять ключевые слова (есть разные модели для keywords extraction), отфильтровывать их в зависимости от тематики отзывов в целом(вероятно, вручную) и использовать модель spacy, которая выделяет зависимости (синатксис). Необходимо брать только зависимые от ключевых слов для каждого отзыва. Тогда отзывы о фильмах по типу (утрированный пример): \"Я недавно купил хорошую машину, а фильм, кстати, ужасный\" будет отнесен к отрицательным, так как \"хороший\" о машине здесь нерелевантен. (Действуя напрямую, как сделано выше, этот отзыв посчитается за нейтральный.)"
      ],
      "metadata": {
        "id": "Lakx1-I1MEmo"
      },
      "id": "Lakx1-I1MEmo"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}